{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: From Application to Theory\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "Imagine you're faced with a decision-making problem where you need to classify something into one of two categories. It might be determining if a bank loan should be approved or denied, or predicting if a student will pass or fail a test based on their study hours. You collect the necessary data, and your task is to find a way to make sense of it to arrive at a clear decision.\n",
    "\n",
    "Logistic regression is the tool that can help you with this. Unlike linear regression, which is used to predict continuous values (values that can fall within a range, such as temperatures or prices), logistic regression is designed for binary decisions: yes or no, true or false, 1 or 0.\n",
    "\n",
    "At its core, logistic regression works by taking your input data and transforming it into a probability, a chance that the answer is 'yes' or 'no'. It doesn't just draw a straight line through the data points but shapes a curve called the sigmoid function that best represents the relationship between the variables.\n",
    "\n",
    "Once you have found this mathematical expression, you can use it to make predictions with confidence. You'll know not just whether the answer is 'yes' or 'no', but also how confident you can be in that classification.\n",
    "\n",
    "Logistic regression is a powerful yet accessible tool in statistics and machine learning. It's not just about crunching numbers; it's about understanding probabilities and making decisions that are informed, precise, and reflective of the complexity of real-world situations.\n",
    "\n",
    "## 2 Application\n",
    "\n",
    "### 2.1 Logistic Regression Application Introduction - Sentiment Analysis\n",
    "\n",
    "Sentiment Analysis is a powerful technique that analyzes the emotions and opinions within text. In the context of movies, it can be applied to understand how audiences feel about a particular film based on their reviews. By applying Logistic Regression to this problem, we can categorize reviews as positive or negative, providing key insights for filmmakers, critics, and audiences.\n",
    "\n",
    "### 2.2 Data Collection\n",
    "\n",
    "For our application, we can obtain data from datasets specifically created for sentiment analysis. The dataset we'll be working with is the Large Movie Review Dataset, also known as the IMDb dataset. This is a comprehensive collection of 50,000 movie reviews, evenly split between positive and negative sentiments. It's publicly available and can be downloaded from the [Stanford AI Lab](http://ai.stanford.edu/~amaas/data/sentiment/). This has already for this application and is available in the [aclImdb](./aclImdb/README) directory.\n",
    "\n",
    "Now let's load this data into our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TRAIN_PATH = './aclImdb/train'\n",
    "TEST_PATH = './aclImdb/test'\n",
    "# Load the movie review data\n",
    "def load_movie_review_data(path):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for sentiment in ['neg', 'pos']:\n",
    "        folder = os.path.join(path, sentiment)\n",
    "        for filename in os.listdir(folder):\n",
    "            with open(os.path.join(folder, filename), 'r') as file:\n",
    "                reviews.append(file.read())\n",
    "                labels.append(sentiment == 'pos')\n",
    "    return reviews, labels\n",
    "\n",
    "reviews, labels = load_movie_review_data(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we created a function that navigates into the folder with the movie reviews we downloaded and extract the contents of the positive and negative reviews.\n",
    "\n",
    "### 2.3 Preprocessing the Data\n",
    "\n",
    "Now that we've loaded the data, we need to preprocess it before training our model.\n",
    "\n",
    "#### 2.3.1 Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into individual units, commonly known as tokens. In most cases, these tokens are words. Tokenization helps in analyzing the frequency and importance of individual words in the text.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'movies', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/santiagogomez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download as nltk_download\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Download the necessary NLTK data for tokenization\n",
    "nltk_download('punkt')\n",
    "text = \"I love movies.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this example to understand what just happened.  \n",
    "\n",
    "##### 2.3.1.1 NLTK\n",
    "\n",
    "NLTK is a package for building python programs to work with human language data. `nltk.download` is a function that makes it easy to download various resources, models, etc., for different NLTK functionalities. These resources are stored online, and the download function fetches them to be used locally.\n",
    "\n",
    "##### 2.3.1.2 punkt\n",
    "\n",
    "`punkt` is a tokenizer model used for unsupervised machine learning tokenization. It's a pre-trained model that knows how to tokenize sentences in different European languages. We downloaded this model when we called:\n",
    "```python\n",
    "nltk_download('punkt')\n",
    "```\n",
    "\n",
    "##### 2.3.1.3 word_tokenize\n",
    "`word_tokenize` is a function that breaks input text into words, which is a common task in Natural Language Processing (NLP). It uses the Punkt tokenizer to perform this task. It uses the pre-trained Punkt tokenizer model to accurately split text into sentences or words. Without it, the `word_tokenize` function doesn't have the necessary knowledge to perform the tokenization.\n",
    "\n",
    "##### 2.3.1.4 Example Summary\n",
    "In our code example above, we downloaded the necessary pre-trained tokenizer model called Punkt which is used by NLTK, the package often used when working with human language data. We then called the `word_tokenize` function with the text `'I love movies.'` so we can break down that text into tokens. Lastly we printed out the resulting tokens `['I', 'love', 'movies', '.']`.\n",
    "\n",
    "#### 2.3.2 Removing Stop Words\n",
    "After tokenization, the next step is to remove stop words. Stop words are common words such as 'the', 'is', 'in', which are generally ignored in text data analysis. They don't contribute to the sentiment or meaning of the text, so they are removed to reduce the dimensionality of the data and focus more on informative words.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk_download('stopwords')\n",
    "\n",
    "# create a set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(stop_words)\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we've downloaded a list of precompiled stop words for various languages from the NLTK servers when we ran the line:\n",
    "```python\n",
    "nltk_download('stopwords')\n",
    "``` \n",
    "\n",
    "With this precompiled list of words downloaded, we filered out all of the tokens that were in this list of stop words so that we would only be left with informative words. In this case we filtered out the stop word `'I'` from the list of tokens `['I', 'love', 'movies', '.']`. The result after filtering out the stop words is: `['love', 'movies', '.']`.\n",
    "\n",
    "#### 2.3.3 Vectorization\n",
    "\n",
    "In machine learning, text data can't be fed directly to models since the models require numerical values. Vectorization is the translation process turning words and sentences into numerical forms, making them digestible for the models. Think of it as converting text into numbers, where each word or phrase becomes a unique numerical value. This transformation not only captures individual words but also the underlying meanings and relationships within the text.\n",
    "\n",
    "##### 2.3.3.1 CountVectorizer\n",
    "One common method for vectorizing text data is to use a technique that counts the occurrence of words. We can use `CountVectorizer` from the sklearn library for this purpose. `CountVectorizer` is a feature extraction technique used in Natural Language Processing (NLP) and text mining specifically designed to convert text data into a numerical format by counting word occurrences.\n",
    "\n",
    "For example, let's imagine we have three simple movie reviews:\n",
    "\n",
    "1. \"I love this movie.\"\n",
    "2. \"I hate this movie.\"\n",
    "3. \"This movie is great. I love it.\"\n",
    "\n",
    "Using `CountVectorizer`, we can represent these reviews in the following matrix:\n",
    "\n",
    "|            | \"I\" | \"love\" | \"this\" | \"movie\" | \"hate\" | \"is\" | \"great\" | \"it\" |\n",
    "|------------|:---:|:------:|:------:|:-------:|:------:|:----:|:-------:|:----:|\n",
    "| Review 1:  |  1  |   1    |   1    |    1    |   0    |  0   |    0    |  0   |\n",
    "| Review 2:  |  1  |   0    |   1    |    1    |   1    |  0   |    0    |  0   |\n",
    "| Review 3:  |  1  |   1    |   1    |    1    |   0    |  1   |    1    |  1   |\n",
    "\n",
    "This matrix is now a numerical representation of the text, and it can be used as input for various machine learning models, allowing them to analyze patterns and relationships within the text data.  If we choose to remove the stop words \"I\", \"this\", \"is\", \"it\", we'll get a simpler matrix of reduced dimensionality (less columns):\n",
    "\n",
    "|            | \"love\" | \"movie\" | \"hate\" | \"great\" |\n",
    "|------------|:------:|:-------:|:------:|:-------:|\n",
    "| Review 1:  |   1    |    1    |   0    |    0    |\n",
    "| Review 2:  |   0    |    1    |   1    |    0    |\n",
    "| Review 3:  |   1    |    1    |   0    |    1    |\n",
    "\n",
    "Here's how `CountVectorizer` works:\n",
    "\n",
    "1. **Tokenization**: `CountVectorizer` starts by tokenizing the text, breaking it down into individual words, phrases, symbols, or other meaningful elements known as tokens.\n",
    "   \n",
    "2. **Building a Vocabulary**: `CountVectorizer` then builds a vocabulary by identifying unique tokens across all the documents. Each unique token becomes a feature in the resulting matrix.\n",
    "\n",
    "3. **Counting Occurrences**: For each document, `CountVectorizer` counts the occurrences of each token in the vocabulary. These counts form the values of the matrix.\n",
    "\n",
    "4. **Removing Stop Words (Optional)**: If specified, common words (stop words) that may not contribute to the analysis can be removed. This is done through the `stop_words` parameter.\n",
    "\n",
    "5. **Creating the Matrix**: The final result is a sparse matrix, a matrix mostly filled with zeros, where each row corresponds to a document, and each column corresponds to a unique token from the vocabulary. The value in each cell represents the count of the occurrence of the token in the corresponding document (review). The result is a matrix filled mostly with zeros (sparse) because most documents only contain a small fraction of the entire vocabulary, so many counts will be zero.\n",
    "\n",
    "By utilizing `CountVectorizer` we can combine the previous 2 steps, tokenization and removing stop words, and obtain a numerical representation of our text data that can be directly used in our model.  Let's continue building our sentiment analysis of movie reviews using logistic regression by leveraging `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we've created an instance of the `CountVectorizer` class and specified that the english stop words should be filtered out. We then used this instance, called `vectorizer`, and called the `fit_transform` method which does two things to the `reviews`:\n",
    " - `fit`: It learns the vocabulary of all the tokens in the `reviews`.\n",
    " - `transform`: It transforms the `reviews` into a matrix that contains the counts of each token's occurrence in the `reviews`.\n",
    "\n",
    "The result is a matrix of word counts we called `X`. The `X` matrix are our numeric features. We'll now take this to split the data into testing and training sets so we can train and later validate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took our feature data `X`, which is a matrix of how often each word appears in each of the movie reviews we downloaded, and our target data `labels`, which contains the result of each review, whether it's a positive review or a negative review, and split each into testing and training sets of data.\n",
    "\n",
    "### 2.4 Building the Logistic Regression Model\n",
    "\n",
    "Now that our data has been preprocessed and we've obtained our training and testing sets, it's time to build our Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `model.fit`, we're training our model on the provided training data. It will iteratively learn the relationship between the words in the reviews (`X_train`) and their corresponding sentiments (`y_train`). `max_iter=10000` simply means we're giving the model enough time to learn from our data. \n",
    "\n",
    "We can then use the trained model to make predictions on our testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is now ready, and we can proceed to analyze and evaluate its performance.\n",
    "\n",
    "### 2.5 Analysis of the Logistic Regression Model\n",
    "\n",
    "#### 2.5.1 Predicting Sentiment for New Reviews\n",
    "\n",
    "Having trained the model using movie reviews and their corresponding sentiments, we can now apply this model to predict the sentiment of new reviews.\n",
    "\n",
    "Suppose you have a new review and you want to know whether it's positive or negative. Here's how you can predict the sentiment using our trained Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This movie was fantastic!\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Text: This movie was awful!\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Text: I didn't think this movie was amazing as many reviews have stated. In fact, I thought it was mediocre at best!\n",
      "Predicted sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predictSentiment(text, sentiment_model = model, vectorize = vectorizer):\n",
    "    # Transforming the text so that it matches the training data\n",
    "    text_vectorized = vectorize.transform([text])\n",
    "    # Predicting the sentiment\n",
    "    predicted_sentiment = sentiment_model.predict(text_vectorized)\n",
    "    # Decoding the predicted sentiment\n",
    "    if predicted_sentiment[0] == 1:\n",
    "        print(f\"Text: {text}\\nPredicted sentiment: Positive\\n\")\n",
    "    else:\n",
    "        print(f\"Text: {text}\\nPredicted sentiment: Negative\\n\")\n",
    "\n",
    "predictSentiment(\"This movie was fantastic!\")\n",
    "predictSentiment(\"This movie was awful!\")\n",
    "predictSentiment(\"I didn't think this movie was amazing as many reviews have stated. In fact, I thought it was mediocre at best!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have shown with the code above, the trained model can be applied to any new reviews, transforming them into the same vectorized form using the `CountVectorizer` and then classifying them with the `LogisticRegression` model.\n",
    "\n",
    "This process ensures that the new reviews are treated consistently with the training data, enabling reliable predictions.\n",
    "\n",
    "However, some of the predictions may not always be quite what you expect. Let's analyze our logistic regression model to further understand how it's working with our movie reviews data.\n",
    "\n",
    "#### 2.5.2 Understanding the Coefficients\n",
    "\n",
    "In Logistic Regression, the relationship between the features (words in the reviews) and the target (sentiment) is quantified using coefficients. The coefficients of our model represent the weight or importance assigned to each word or feature in determining the sentiment of a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.38859414e-01  1.88130377e-01  0.00000000e+00 ...  1.32824756e-05\n",
      "  7.53057485e-03 -3.32844842e-02]\n"
     ]
    }
   ],
   "source": [
    "coefficients = model.coef_[0]\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the coefficients, we can understand how each word in the reviews contributes to the predicted sentiment. Here's how to print the top coefficients:\n",
    "\n",
    "1. We first need to get the actual words. To achieve this we use the `vectorizer.get_feature_names_out()` function. This function retrieves the feature names, which are the words in the vocabulary that were vectorized during the preprocessing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We then need to pair up the coefficients with their respective feature names. We use `zip` for this purpose as it's a function that generates tuples. In the code below, `zip` combines the coefficients with the corresponding feature names. Since both `coefficients` and `feature_names` are lists that correspond to each other (meaning the $n^\\text{th}$ element in `coefficients` corresponds to the $n^\\text{th}$ word in `feature_names`), using zip creates pairs that maintain this relationship. For example, if you had `coefficients = [0.5, -0.3, 1.2]` and `feature_names = ['happy', 'sad', 'great']`, using `zip(coefficients, feature_names)` would return the pairs:\n",
    "`(0.5, 'happy')`, `(-0.3, 'sad')`, and `(1.2, 'great')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name_coefficient_pairs = zip(coefficients, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now that we have the feature names with their respective coefficients, we can sort them based on the coefficient to see which values have the greatest influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sort the coefficients\n",
    "def sort_key(coefficient_word_tuple):\n",
    "    coefficient, _ = coefficient_word_tuple\n",
    "    return coefficient\n",
    "\n",
    "# sort the coefficients from the smallest to the largest\n",
    "sorted_coefficients = sorted(feature_name_coefficient_pairs, key=sort_key, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see the top positive coefficients and the top negative coefficients to see which top words the model has learned to associate with a positive sentiment (in the case of a positive coefficient) and a negative sentiment (in the case of a negative coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Positive Coefficients:\n",
      "wonderfully: 1.328571350909191\n",
      "perfect: 1.3007026200487972\n",
      "excellent: 1.282701882392441\n",
      "funniest: 1.2737573405623213\n",
      "rare: 1.2669243006536708\n",
      "surprisingly: 1.230629278284308\n",
      "perfectly: 1.2222360392311848\n",
      "erotic: 1.1934035648995072\n",
      "finest: 1.1907394698705622\n",
      "jolie: 1.1778307043962923\n",
      "\n",
      "Top Negative Coefficients:\n",
      "badly: -1.3633704619772233\n",
      "horrible: -1.3918862413007114\n",
      "worse: -1.396960038007195\n",
      "disappointing: -1.4357959426869529\n",
      "awful: -1.6746433115917152\n",
      "lacks: -1.6939809662107006\n",
      "poorly: -1.8569035814037598\n",
      "waste: -2.2241164925242907\n",
      "worst: -2.23799666913952\n",
      "disappointment: -2.242382523387833\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Positive Coefficients:\")\n",
    "for coef, word in sorted_coefficients[:10]:\n",
    "    print(f\"{word}: {coef}\")\n",
    "\n",
    "print(\"\\nTop Negative Coefficients:\")\n",
    "for coef, word in sorted_coefficients[-10:]:\n",
    "    print(f\"{word}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned the words with the positive coefficients are the ones that usually appear in positive reviews, which means they contribute to a prediction of positive sentiment. Similarly, the model has learned that the words with negative coefficients are the ones that usually appear in a negative review. Since these are the top coefficients, it means that these words have a strong association with their respective sentiment. These are the words that the model considers most influential in determining the sentiment of a review.\n",
    "\n",
    "#### 2.5.3 Checking for Overfitting or Underfitting\n",
    "\n",
    "We want our model to perform well not just on the training data but also on unseen data. This requires avoiding overfitting (too complex) and underfitting (too simple).\n",
    "\n",
    "As discussed in [Linear Regression - Deeper Dive sections 2.3 and 2.4](../linear-regression/Linear%20Regression%20-%20Deeper%20Dive.ipynb), Overfitting is when a model performs exceptionally well on training data but poorly on unseen data and underfitting is when the model performs poorly on both.\n",
    "\n",
    "We can check for overfitting and underfitting by comparing the accuracy on the training and testing datasets. Below, we compare how well the model model is predicting the sentiments of the reviews it was trained on, `train_accuracy`, and how well the model is predicting the sentiments of reviews it hasn't seen. The accuracy on the test data, `test_accuracy`, provides a measure of how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9987\n",
      "Testing Accuracy: 0.8778\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant difference between the training and testing accuracies might indicate overfitting. Similarly, poor performance on both may signal underfitting. Cross-validation, as mentioned in [Linear Regression - Deeper Dive section 2.6.1](../linear-regression/Linear%20Regression%20-%20Deeper%20Dive.ipynb), can further assist in detecting these issues.\n",
    "\n",
    "### 2.6 Evaluating the Model\n",
    "\n",
    "Evaluating the model involves using various metrics to understand how well the model is performing.\n",
    "\n",
    "- Confusion Matrix\n",
    "- Accurary\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "- ROC Curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[2216  322]\n",
      " [ 289 2173]]\n",
      "Accuracy: 0.8778\n",
      "Precision: 0.8709418837675351\n",
      "Recall: 0.8826157595450853\n",
      "F1 Score: 0.8767399636877143\n",
      "fpr [0.         0.12687155 1.        ]\n",
      "tpr [0.         0.88261576 1.        ]\n",
      "auc 0.877872103570809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Metrics\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n",
    "\n",
    "# ROC and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print('fpr', fpr)\n",
    "print('tpr', tpr)\n",
    "print('auc', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Improving the Model\n",
    "\n",
    "### 2.8 Application Conclusion\n",
    "\n",
    "### 3 Theory\n",
    "\n",
    "#### 3.1 Introduction to the Theory\n",
    "\n",
    "#### 3.2 Understanding the Sigmoid Function\n",
    "\n",
    "#### 3.3 Coefficients and Odds Ratios\n",
    "\n",
    "#### 3.4 Finding Parameters\n",
    "- Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "#### 3.5 Cost Function\n",
    "- Log Loss\n",
    "\n",
    "#### 3.6 Gradient Descent\n",
    "\n",
    "#### 3.6.1 Equations \n",
    "- Equations for updating parameters\n",
    "\n",
    "#### 3.6.2 Example\n",
    "- Step by Step walk-through\n",
    "\n",
    "### 3.7 Decision Boundary\n",
    "- Understanding how decisions are made\n",
    "\n",
    "### 4 Advanced Concepts\n",
    "\n",
    "#### 4.1 Mathematical Foundations\n",
    "\n",
    "#### 4.2 Regularization Techniques\n",
    "\n",
    "#### 4.3 Multiclass Classification\n",
    "\n",
    "### 5 Conclusion\n",
    "\n",
    "### 5.1 Next Steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
